{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615f7ff5-a060-4ad4-acef-572709d2ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "# from streamlit_webrtc import webrtc_streamer\n",
    "# import av\n",
    "# import cv2 \n",
    "# import numpy as np \n",
    "# import mediapipe as mp \n",
    "# from keras.models import load_model\n",
    "# import webbrowser\n",
    "\n",
    "# model  = load_model(r\"model.h5\")\n",
    "# label = np.load(r\"labels.npy\")\n",
    "# holistic = mp.solutions.holistic\n",
    "# hands = mp.solutions.hands\n",
    "# holis = holistic.Holistic()\n",
    "# drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# st.header(\"Emotion Based Music Recommender\")\n",
    "\n",
    "# if \"run\" not in st.session_state:\n",
    "# \tst.session_state[\"run\"] = \"true\"\n",
    "\n",
    "# try:\n",
    "# \temotion = np.load(\"emotion.npy\")[0]\n",
    "# except:\n",
    "# \temotion=\"\"\n",
    "\n",
    "# if not(emotion):\n",
    "# \tst.session_state[\"run\"] = \"true\"\n",
    "# else:\n",
    "# \tst.session_state[\"run\"] = \"false\"\n",
    "\n",
    "# class EmotionProcessor:\n",
    "# \tdef recv(self, frame):\n",
    "# \t\tfrm = frame.to_ndarray(format=\"bgr24\")\n",
    "\n",
    "# \t\t##############################\n",
    "# \t\tfrm = cv2.flip(frm, 1)\n",
    "\n",
    "# \t\tres = holis.process(cv2.cvtColor(frm, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# \t\tlst = []\n",
    "\n",
    "# \t\tif res.face_landmarks:\n",
    "# \t\t\tfor i in res.face_landmarks.landmark:\n",
    "# \t\t\t\tlst.append(i.x - res.face_landmarks.landmark[1].x)\n",
    "# \t\t\t\tlst.append(i.y - res.face_landmarks.landmark[1].y)\n",
    "\n",
    "# \t\t\tif res.left_hand_landmarks:\n",
    "# \t\t\t\tfor i in res.left_hand_landmarks.landmark:\n",
    "# \t\t\t\t\tlst.append(i.x - res.left_hand_landmarks.landmark[8].x)\n",
    "# \t\t\t\t\tlst.append(i.y - res.left_hand_landmarks.landmark[8].y)\n",
    "# \t\t\telse:\n",
    "# \t\t\t\tfor i in range(42):\n",
    "# \t\t\t\t\tlst.append(0.0)\n",
    "\n",
    "# \t\t\tif res.right_hand_landmarks:\n",
    "# \t\t\t\tfor i in res.right_hand_landmarks.landmark:\n",
    "# \t\t\t\t\tlst.append(i.x - res.right_hand_landmarks.landmark[8].x)\n",
    "# \t\t\t\t\tlst.append(i.y - res.right_hand_landmarks.landmark[8].y)\n",
    "# \t\t\telse:\n",
    "# \t\t\t\tfor i in range(42):\n",
    "# \t\t\t\t\tlst.append(0.0)\n",
    "\n",
    "# \t\t\tlst = np.array(lst).reshape(1,-1)\n",
    "\n",
    "# \t\t\tpred = label[np.argmax(model.predict(lst))]\n",
    "\n",
    "# \t\t\tprint(pred)\n",
    "# \t\t\tcv2.putText(frm, pred, (50,50),cv2.FONT_ITALIC, 1, (255,0,0),2)\n",
    "\n",
    "# \t\t\tnp.save(\"emotion.npy\", np.array([pred]))\n",
    "\n",
    "\t\t\t\n",
    "# \t\tdrawing.draw_landmarks(frm, res.face_landmarks, holistic.FACEMESH_TESSELATION,\n",
    "# \t\t\t\t\t\t\t\tlandmark_drawing_spec=drawing.DrawingSpec(color=(0,0,255), thickness=-1, circle_radius=1),\n",
    "# \t\t\t\t\t\t\t\tconnection_drawing_spec=drawing.DrawingSpec(thickness=1))\n",
    "# \t\tdrawing.draw_landmarks(frm, res.left_hand_landmarks, hands.HAND_CONNECTIONS)\n",
    "# \t\tdrawing.draw_landmarks(frm, res.right_hand_landmarks, hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "# \t\t##############################\n",
    "\n",
    "# \t\treturn av.VideoFrame.from_ndarray(frm, format=\"bgr24\")\n",
    "\n",
    "# lang = st.text_input(\"Language\")\n",
    "# singer = st.text_input(\"singer\")\n",
    "\n",
    "# if lang and singer and st.session_state[\"run\"] != \"false\":\n",
    "# \twebrtc_streamer(key=\"key\", desired_playing_state=True,\n",
    "# \t\t\t\tvideo_processor_factory=EmotionProcessor)\n",
    "\n",
    "# btn = st.button(\"Recommend me songs\")\n",
    "\n",
    "# if btn:\n",
    "# \tif not(emotion):\n",
    "# \t\tst.warning(\"Please let me capture your emotion first\")\n",
    "# \t\tst.session_state[\"run\"] = \"true\"\n",
    "# \telse:\n",
    "# \t\twebbrowser.open(f\"https://www.youtube.com/results?search_query={lang}+{emotion}+song+{singer}\")\n",
    "# \t\tnp.save(\"emotion.npy\", np.array([\"\"]))\n",
    "# \t\tst.session_state[\"run\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b47407e-6437-493e-8adb-e92051482bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import mediapipe as mp\n",
    "# from keras.models import load_model\n",
    "# import webbrowser\n",
    "\n",
    "# # Load the trained model and labels\n",
    "# model = load_model(\"model.h5\")\n",
    "# labels = np.load(\"labels.npy\")  # Ensure labels.npy contains all emotions\n",
    "\n",
    "# # Initialize MediaPipe Holistic model\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# holistic = mp_holistic.Holistic(static_image_mode=False, model_complexity=1, smooth_landmarks=True)\n",
    "\n",
    "# # Define possible emotions\n",
    "# EMOTIONS = [\"Happy\", \"Sad\", \"Angry\", \"Surprised\", \"Neutral\", \"Disgusted\", \"Fearful\"]\n",
    "\n",
    "# def extract_landmarks(results):\n",
    "#     \"\"\"Extracts face and hand landmarks from MediaPipe results.\"\"\"\n",
    "#     landmarks = []\n",
    "\n",
    "#     # Face landmarks (468 points)\n",
    "#     if results.face_landmarks:\n",
    "#         for lm in results.face_landmarks.landmark:\n",
    "#             landmarks.append(lm.x)\n",
    "#             landmarks.append(lm.y)\n",
    "#     else:\n",
    "#         landmarks.extend([0.0] * 936)  # Fill with zeros if no face detected\n",
    "\n",
    "#     # Left-hand landmarks (21 points)\n",
    "#     if results.left_hand_landmarks:\n",
    "#         for lm in results.left_hand_landmarks.landmark:\n",
    "#             landmarks.append(lm.x)\n",
    "#             landmarks.append(lm.y)\n",
    "#     else:\n",
    "#         landmarks.extend([0.0] * 42)  # Fill with zeros\n",
    "\n",
    "#     # Right-hand landmarks (21 points)\n",
    "#     if results.right_hand_landmarks:\n",
    "#         for lm in results.right_hand_landmarks.landmark:\n",
    "#             landmarks.append(lm.x)\n",
    "#             landmarks.append(lm.y)\n",
    "#     else:\n",
    "#         landmarks.extend([0.0] * 42)  # Fill with zeros\n",
    "\n",
    "#     return np.array(landmarks).reshape(1, -1)  # Ensure shape is (1, 1020)\n",
    "\n",
    "# def detect_emotion():\n",
    "#     cap = cv2.VideoCapture(0)  # Open webcam\n",
    "#     detected_emotion = \"Neutral\"  # Default emotion if none detected\n",
    "\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         # Convert frame to RGB for MediaPipe processing\n",
    "#         rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         results = holistic.process(rgb_frame)\n",
    "\n",
    "#         # Extract facial landmarks for prediction\n",
    "#         landmarks = extract_landmarks(results)\n",
    "\n",
    "#         if landmarks.shape[1] == 1020:  # Ensure correct input shape\n",
    "#             pred_index = np.argmax(model.predict(landmarks))\n",
    "#             detected_emotion = EMOTIONS[pred_index]  # Get emotion label\n",
    "\n",
    "#             # Display emotion on the video feed\n",
    "#             cv2.putText(frame, detected_emotion, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "#                         1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "#         cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break  # Press 'q' to quit\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     return detected_emotion\n",
    "\n",
    "# # Capture emotion from live camera\n",
    "# emotion = detect_emotion()\n",
    "# print(f\"Detected Emotion: {emotion}\")\n",
    "\n",
    "# # Get user input\n",
    "# if emotion:\n",
    "#     language = input(\"Enter Language: \")\n",
    "#     singer = input(\"Enter Singer: \")\n",
    "\n",
    "#     # Open YouTube with recommended songs based on detected emotion\n",
    "#     url = f\"https://www.youtube.com/results?search_query={language}+{emotion}+song+{singer}\"\n",
    "#     print(f\"Opening: {url}\")\n",
    "#     webbrowser.open(url)\n",
    "# else:\n",
    "#     print(\"No emotion detected. Try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5141aee8-98a0-4c87-91da-f7794f5706f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRAJES DAS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expected input shape (None, 1020), but got (1, 48, 48, 1). Check model training data.\n",
      "No emotion detected. Try again.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained emotion detection model\n",
    "model = load_model(r\"model.h5\")  # Ensure the model file exists\n",
    "EMOTIONS = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "\n",
    "# Load OpenCV face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + r\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def detect_emotion():\n",
    "    cap = cv2.VideoCapture(0)  # Open webcam\n",
    "    detected_emotion = \"Neutral\"  # Default emotion if none detected\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Couldn't capture frame from webcam.\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w]  # Crop the face\n",
    "            roi_gray = cv2.resize(roi_gray, (48, 48))  # Resize to match model input\n",
    "            roi_gray = roi_gray.astype(\"float32\") / 255.0  # Normalize\n",
    "            roi_gray = np.expand_dims(roi_gray, axis=0)  # Add batch dimension\n",
    "            roi_gray = np.expand_dims(roi_gray, axis=-1)  # Add channel dimension\n",
    "\n",
    "            # Ensure the model input shape matches\n",
    "            expected_shape = model.input_shape\n",
    "            if expected_shape[1:] != (48, 48, 1):\n",
    "                print(f\"Model expected input shape {expected_shape}, but got {roi_gray.shape}. Check model training data.\")\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                return None\n",
    "\n",
    "            # Get predicted emotion\n",
    "            pred_index = np.argmax(model.predict(roi_gray))\n",
    "            detected_emotion = EMOTIONS[pred_index]\n",
    "\n",
    "            # Draw rectangle and label\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, detected_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return detected_emotion\n",
    "\n",
    "# Capture emotion from live camera\n",
    "emotion = detect_emotion()\n",
    "\n",
    "if emotion:\n",
    "    print(f\"Detected Emotion: {emotion}\")\n",
    "    language = input(\"Enter Language: \").strip()\n",
    "    singer = input(\"Enter Singer: \").strip()\n",
    "\n",
    "    # Open YouTube with recommended songs based on detected emotion\n",
    "    search_query = f\"{language} {emotion} song {singer}\".replace(\" \", \"+\")\n",
    "    url = f\"https://www.youtube.com/results?search_query={search_query}\"\n",
    "    print(f\"Opening: {url}\")\n",
    "    webbrowser.open(url)\n",
    "else:\n",
    "    print(\"No emotion detected. Try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e979850-90e1-415e-9d73-b0e6dca3261a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a552f-fe0f-4857-9c97-b9f67a67094d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
